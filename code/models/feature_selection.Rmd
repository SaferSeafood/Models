---
title: "Feature Selection with Projpred"
author: "SaferSeafood"
date: "2024-04-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE) 

library(projpred)
library(tidyverse)
library(rfishbase)
library(tidymodels)
library(multilevelmod)
library(rstanarm)
library(posterior)
``` 

## Overview

This document contains code for feature selection using the `projpred` package. To use this package for feature selection, we need to set a reference model and then test combinations of predictors to see the minimum amount of predictors that has the same model accuracy.

The process of the feature selection was based off of `projpred` documentation.

We performed feature selection on our final model selection with the formula including: diet, habitat, species, year, sediment DDT (including interacting predictors as well)

### Load in Data
 
Load in appropriate dataset and split data into training and testing sets.
```{r}
# read in cleaned data
fish_cleaned <- read_csv(here::here("data", "data_outputs", "ddx_southernCA_norm.csv"))

# split new data
fish_split <- initial_split(fish_cleaned, prop = 0.7)
fish_train <- training(fish_split)
fish_test <- testing(fish_split)
```

### Set reference model 

Set reference model using stan_glmer() function but the same formula as the clients: 
This did not work unless I selected only the predictors in the model so tidy the data to select these first. 

```{r}
# Clean training dataset to only include predictors in model
fish_projpred_train <- fish_train %>% 
  select(TotalDDT.trans.non, TotalDDT.sed.trans, trophic_category, feeding_position, Year, CompositeCommonName)

# set the reference model as specified in projpred documentation
refm_fit <- stan_glmer(
  TotalDDT.trans.non ~ TotalDDT.sed.trans * trophic_category + TotalDDT.sed.trans * feeding_position + Year + (1|CompositeCommonName),
  family = gaussian(),
  data = fish_projpred_train,
  refresh = 0,
  prior_intercept = rstanarm::cauchy(0, 0.5)
)
```

### Incorporating projpred

The next steps all include the workflow from the projpred vignette from the projpred documentation.

In the projpred vignette, they set the method to L1, but it did not work with our multilevel model, so I took it out.
Plot the model performance using rmse. 

```{r}
# set the reference model (client's model)
refm_obj <- get_refmodel(refm_fit)

# preliminary (fast) run
# Preliminary cv_varsel() run:
# had to take out method = L1 because reference model is different format
cvvs_fast <- cv_varsel(
  refm_obj,
  validate_search = FALSE,
  ### Only for the sake of speed (not recommended in general):
  refit_prj = FALSE,
  ###
  nterms_max = 15,
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs_fast, stats = "rmse", ranking_nterms_max = NA)
```

```{r}
# Preliminary cv_varsel() run with `refit_prj = TRUE`:
cvvs_fast_refit <- cv_varsel(
  cvvs_fast,
  ### Only for the sake of speed (not recommended in general):
  nclusters_pred = 15,
  ###
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs_fast_refit, stats = "rmse", ranking_nterms_max = NA)
```
Both plot shows that the model performance levels at around 3 predictors. 

We now run the model with k-fold cross validation (using 10 folds).
```{r}
ncores <- parallel::detectCores(logical = FALSE)

# Refit the reference model K times:
cv_fits <- run_cvfun(
  refm_obj,
  K = 10
)

```

```{r}
# For running projpred's CV in parallel (see cv_varsel()'s argument `parallel`):
doParallel::registerDoParallel(ncores)
# Final cv_varsel() run:
cvvs <- cv_varsel(
  refm_obj,
  cv_method = "kfold",
  cvfits = cv_fits,
  ### Only for the sake of speed (not recommended in general):
  nclusters_pred = 20,
  ###
  nterms_max = 5,
  parallel = TRUE,
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs, stats = "rmse", deltas = TRUE)

suggest_size(cvvs, stat = "rmse")
```

Ranking predictors

```{r}
rk <- ranking(cvvs)

( pr_rk <- cv_proportions(rk) )

plot(pr_rk)
```

plotting posteriors

```{r warning = FALSE, message = FALSE}
# selected optimal number of predictors
( predictors_final <- head(rk[["fulldata"]], 7) )

prj <- project(
  refm_obj,
  predictor_terms = predictors_final,
  verbose = TRUE
)

prj_mat <- as.matrix(prj)

prj_drws <- as_draws_matrix(prj_mat)
prj_smmry <- summarize_draws(
  prj_drws,
  "median", "mad", function(x) quantile(x, probs = c(0.025, 0.975))
)
# Coerce to a `data.frame` because pkgdown versions > 1.6.1 don't print the
# tibble correctly:
prj_smmry <- as.data.frame(prj_smmry)
print(prj_smmry, digits = 1)

library(bayesplot)
bayesplot_theme_set(ggplot2::theme_bw())
mcmc_intervals(prj_mat) +
  ggplot2::coord_cartesian(xlim = c(-1.5, 1.6))
```

predictions
```{r}
prj_linpred <- proj_linpred(prj, newdata = fish_projpred_train)
cbind(fish_projpred_train, linpred = as.vector(prj_linpred$pred))

prj_predict <- proj_predict(prj)
# Using the 'bayesplot' package:
ppc_dens_overlay(y = fish_projpred_train$TotalDDT.trans.non, yrep = prj_predict)
```

### Results/overview of projpred

The predictor ranking chart shows that all predictors are likely important, so the model will be left as is. 


