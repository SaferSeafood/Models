---
title: "Feature Selection with Projpred"
author: "SaferSeafood"
date: "2024-04-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(projpred)
library(tidyverse)
library(rfishbase)
library(tidymodels)
library(multilevelmod)
library(rstanarm)
library(posterior)
``` 

## Overview

This document contains code for feature selection using the `projpred` package. To use this package for feature selection, we need to set a reference model and then test combinations of predictors to see the minimum amount of predictors that has the same model accuracy.

The process of the feature selection was based off of `projpred` documentation.

### Load in Data

```{r}
# read in cleaned data
fish_cleaned <- read_csv(here::here("data", "data_outputs", "ddx_southernCA_norm.csv"))
```

***Add family to the data***

```{r}
# split new data
fish_split <- initial_split(fish_cleaned, prop = 0.7)
fish_train <- training(fish_split)
fish_test <- testing(fish_split)
```

### Set reference model 

Set reference model using stan_glmer() function but the same formula as the clients: 
This did not work unless selected only the predictors in the model so tidy the data to select these.

```{r}
# create training dataset 
fish_projpred_train <- fish_train %>% 
  select(TotalDDT.trans.non, TotalDDT.sed.trans, trophic_category, feeding_position, Year, CompositeCommonName)

# set reference model
refm_fit <- stan_glmer(
  TotalDDT.trans.non ~ TotalDDT.sed.trans * trophic_category + TotalDDT.sed.trans * feeding_position + Year + (1|CompositeCommonName),
  family = gaussian(),
  data = fish_train,
  refresh = 0,
  prior_intercept = rstanarm::cauchy(0, 0.5)
)
```

### Incorporating projpred

In the projpred vignette, they set the method to L1, but it did not work with our multilevel model, so I took it out.
Plot the model performance using rmse. 

```{r}
# set reference model (client's model)
refm_obj <- get_refmodel(refm_fit)

# preliminary (fast) run
# Preliminary cv_varsel() run:
# had to take out method = L1 because reference model is different format
cvvs_fast <- cv_varsel(
  refm_obj,
  validate_search = FALSE,
  ### Only for the sake of speed (not recommended in general):
  refit_prj = FALSE,
  ###
  nterms_max = 15,
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs_fast, stats = "rmse", ranking_nterms_max = NA)
```

```{r}
# Preliminary cv_varsel() run with `refit_prj = TRUE`:
cvvs_fast_refit <- cv_varsel(
  cvvs_fast,
  ### Only for the sake of speed (not recommended in general):
  nclusters_pred = 15,
  ###
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs_fast_refit, stats = "rmse", ranking_nterms_max = NA)
```

run the model with k-fold cross validation.
```{r}
ncores <- parallel::detectCores(logical = FALSE)

# Refit the reference model K times:
cv_fits <- run_cvfun(
  refm_obj,
  K = 10
)

```

```{r}
# For running projpred's CV in parallel (see cv_varsel()'s argument `parallel`):
doParallel::registerDoParallel(ncores)
# Final cv_varsel() run:
cvvs <- cv_varsel(
  refm_obj,
  cv_method = "kfold",
  cvfits = cv_fits,
  ### Only for the sake of speed (not recommended in general):
  nclusters_pred = 20,
  ###
  nterms_max = 5,
  parallel = TRUE,
  ### In interactive use, we recommend not to deactivate the verbose mode:
  verbose = FALSE
  ### 
)

plot(cvvs, stats = "rmse", deltas = TRUE)

suggest_size(cvvs, stat = "rmse")
```

Ranking predictors

```{r}
rk <- ranking(cvvs)

( pr_rk <- cv_proportions(rk) )

plot(pr_rk)
```

plotting posteriors

```{r warning = FALSE, message = FALSE}
# selected optimal number of predictors
( predictors_final <- head(rk[["fulldata"]], 7) )

prj <- project(
  refm_obj,
  predictor_terms = predictors_final,
  verbose = TRUE
)

prj_mat <- as.matrix(prj)

prj_drws <- as_draws_matrix(prj_mat)
prj_smmry <- summarize_draws(
  prj_drws,
  "median", "mad", function(x) quantile(x, probs = c(0.025, 0.975))
)
# Coerce to a `data.frame` because pkgdown versions > 1.6.1 don't print the
# tibble correctly:
prj_smmry <- as.data.frame(prj_smmry)
print(prj_smmry, digits = 1)

library(bayesplot)
bayesplot_theme_set(ggplot2::theme_bw())
mcmc_intervals(prj_mat) +
  ggplot2::coord_cartesian(xlim = c(-1.5, 1.6))

refm_mat <- as.matrix(refm_fit)
mcmc_intervals(refm_mat, pars = colnames(prj_mat)) +
  ggplot2::coord_cartesian(xlim = c(-1.5, 1.6))
```

predictions
```{r}

prj_linpred <- proj_linpred(prj, newdata = fish_projpred_train)
cbind(fish_projpred_train, linpred = as.vector(prj_linpred$pred))

prj_predict <- proj_predict(prj)
# Using the 'bayesplot' package:
ppc_dens_overlay(y = fish_projpred_train$TotalDDT.trans.non, yrep = prj_predict)
```

### Results/overview

The interacting predictors MAY not be needed and may have been leading to overfitting. We will run tidymodels without these predictors and see if there is better generalizability. 
